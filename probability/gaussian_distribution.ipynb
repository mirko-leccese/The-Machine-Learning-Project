{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229e0416-b855-45d2-a0c5-ed7cdaf5eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import ipywidgets\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "from scipy.integrate import quad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca0baf8-af09-4eac-9ac9-f91f42a5d4f7",
   "metadata": {},
   "source": [
    "# An informal Introduction to the Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04031728-b264-4077-8e3f-c283d2265564",
   "metadata": {},
   "source": [
    "In this notebook, we provide a relatively informal introduction the concept of **probability distribution**. The significance of this subject lies in its foundational role across various disciplines (data science as well as scientic subjects such as physics or chemistry), where understanding the probability distribution shapes of data or phenomena under examination serves as a starting point for most applications. There are lots of mathematical details that could be covered when talking about probability distributions, since the subject is vast and can be discussed under different persepctives. The aim of this notebook is to provide just an introductory overview, but without compromising too heavily on the mathematical aspects. Specifically, after a brief introduction on general probability distributions, we will delve into one of the most important probability distribution, namely the so-called **normal or Gaussian Distribution**.  Additionally, we will include Python simulations to facilitate a better grasp of the concepts discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3eaca-238c-4237-b951-0bb3241c38ec",
   "metadata": {},
   "source": [
    "## Probability Densities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf681e03-8655-4f78-ae5b-a83e7af0e788",
   "metadata": {},
   "source": [
    "In a *frequentistic* approach, we define the probability of an event as a measure of the frequency for the event to occur, in the limit that the total number of trials goes to infinity. For instance, the probabilty of rolling a dice and get \"head\" is $1/2$, since in the limit of an infinite number of trials, we should expect to get \"head\" half of the time and \"tail\" the other half. Such probabilities are defined over discrete sets of events, in the provided example $\\{H, T\\}$, i.e. \"head\" or \"tail\". However, most of the time we deal with continuous variables and we need to extend probabilities to such a continuous case. \n",
    "\n",
    "If $x$ is a real-value continuous variable, the *probability density* over $x$ is the defined as the quantity $p(x)$ such that $p(x)\\delta x$ is the probability for $x$ to fall in the interval $(x, x+\\delta x)$, in the limit $\\delta x \\to 0$. Then, we may define:\n",
    "\n",
    "$$P(x \\in (a, b)) = \\int_a^b p(x) dx $$\n",
    "\n",
    "which is the probability for $x$ to fall in the interval $(a, b)$. The probability density $p(x)$ must satisfy the following two conditions:\n",
    "$$ \\begin{gather}\n",
    "p(x) \\geq 0 \\\\\n",
    "\\int_{-\\infty}^{+\\infty} p(x) dx = 1\n",
    "\\end{gather}\n",
    "$$\n",
    "The sum and product rules, as well as Beyes' Theorem, apply equally to the case of probability densities. If $x$ and$y$ are two real variables, then the product and sum rules take the form:\n",
    "$$\\begin{gather}\n",
    "p(x) = \\int p(x,y) dy \\\\\n",
    "p(x,y) = p(y|x) p(x)\n",
    "\\end{gather}\n",
    "$$\n",
    "where $p(y|x)$ is the *conditional probability* of y given x. \n",
    "In analogy with the discrete case, we may define the average value of some function $f(x)$, assuming that  $x$ follows the probability distributions $p(x)$, as:\n",
    "$$\n",
    "\\mathbb{E}[f] = \\int p(x) f(x) dx\n",
    "$$\n",
    "The operator $\\mathbb{E}(\\cdot)$ is generally known as the **expectation** (of $f(x)$). The **variance** of $f(x)$ is instead a quantity measuring the variability in $f(x)$ around its mean value $\\mathbb{E}[f(x)]$ and is defined as:\n",
    "$$\n",
    "\\text{var}[f] = \\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])^2]\n",
    "$$\n",
    "in other words, it is the expectation value of the squared difference between $f(x)$ and its mean value. We take the *squared* difference since the expected value of the mere difference $f(x) - \\mathbb{E}[f(x)]$ is identically vanishing. Calculating explictly the expected value and recalling that $\\mathbb{E}$ is a *linear* operator, we can re-write the variance in a rather simple form: \n",
    "$$\\begin{align}\n",
    "\\text{var}[f(x)] &= \\mathbb{E}[f(x)^2 -2f(x)E[f(x)] + (\\mathbb{E}[f(x)])^2] \\\\\n",
    "&=\\mathbb{E}[f(x)^2] - 2(\\mathbb{E}[f(x)])^2+(\\mathbb{E}[f(x)])^2 \\\\\n",
    "&= \\mathbb{E}[f(x)^2]-\\mathbb{E}[f(x)]^2\n",
    "\\end{align}\n",
    "$$\n",
    "For two random variables $x$ and $y$ we define also the **covariance** as:\n",
    "$$\\begin{align}\n",
    "\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n",
    "&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n",
    "\\end{align}\n",
    "$$\n",
    "which expresses the extent to which $x$ and $y$ vary together (i.e. \"co-vary\"). Indeed, if $x$ and $y$ are independent variables, then:\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{x,y}[xy] &= \\int \\int xy p(x)p(y) dxdy \\\\\n",
    "&= \\left( \\int x p(x) dx)\\right) \\left( \\int y p(y) dy\\right) \\\\\n",
    "&= \\mathbb{E}_x[x]\\mathbb{E}_y[y]\n",
    "\\end{align}\n",
    "$$\n",
    "hence their covariance vanishes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e88ccee-2e19-4a75-9cb5-a460f9db35dd",
   "metadata": {},
   "source": [
    "## The Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdacbc11-83a7-42d6-9a75-77e21e320f88",
   "metadata": {},
   "source": [
    "We start by discussing one of the most important probability distribution in maths and science in general, the so-called **normal** or **Gaussian distribution**. Let first consider the simplest case of a single real-valued variable $x$, for which the Gaussian distribution is defined as:\n",
    "$$\n",
    "\\mathcal{N}(x|\\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left({-\\frac{1}{2\\sigma^2}(x-\\mu)^2}\\right)\n",
    "$$\n",
    "The distribution is governed by two parameters: $\\mu$, called the **mean**, and $\\sigma^2$, called the **variance**. These names will be justified in the following. The square root of variance, i.e. $\\sigma$, is called the **standard deviation**, while the reciprocal value of the variance is generally known as the **precision**, $\\beta = 1/\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6035727-5e1e-4998-bd1e-579e527ff62e",
   "metadata": {},
   "source": [
    "In the cells below, we write a simple function to plot a Gaussian distribution to see how its shape change according to different values of mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ef3b65d-788a-4c4f-8ca8-e11c4608870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_points = 1000\n",
    "x_grid = np.linspace(-100, 100, num_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8651bf13-39f5-462e-9d0a-289b502f6616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gaussian(x: np.array, mu: float, sigma: float) -> np.array:\n",
    "    '''\n",
    "    Function returning Gaussian distribution y values given:\n",
    "\n",
    "    :params x: numpy array defining x values\n",
    "    :params mu: float defining the mean\n",
    "    :params sigma: float defining the variance\n",
    "    '''\n",
    "    a = 1/np.sqrt(2*np.pi*sigma**2)\n",
    "    b = -1/(2*sigma**2)\n",
    "\n",
    "    return a * np.exp(b*(x-mu)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd119dec-0fb7-4074-b416-eaac91683b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian(mu: float , sigma: float):\n",
    "    '''\n",
    "    Function returning the plot of a Gaussian distribution given:\n",
    "\n",
    "    :params mu: its mean\n",
    "    :params sigma: its variance \n",
    "    '''\n",
    "\n",
    "    y = generate_gaussian(x_grid, mu, sigma)\n",
    "\n",
    "    # Plot histograms of sample means\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    fig.suptitle('The Gaussian Distribution')\n",
    "\n",
    "    ax.plot(x_grid, y, color='royalblue', linewidth=2)\n",
    "    # Fill the area under the plot with light blue color\n",
    "    ax.fill_between(x_grid, y, color='lightblue', alpha=0.2)\n",
    "\n",
    "    # Plotting vertical lines at \\mu+\\sigma and \\mu-\\sigma\n",
    "    ax.axvline(mu+sigma, linestyle=\"--\", color=\"red\", linewidth=2)\n",
    "    ax.axvline(mu-sigma, linestyle=\"--\", color=\"red\", linewidth=2)\n",
    "    \n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel(r'$\\mathcal{N}(x|\\mu, \\sigma^2)$')\n",
    "    ax.set_title(fr\"Parameters: $\\mu$ = {mu}, $\\sigma$={round(sigma, 1)}\")\n",
    "\n",
    "    # Add grid lines\n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51d10f6b-afb2-4ae4-8936-ca02f68320b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6799da7203624a54aabd590cf4bc29d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='mu', max=20.0, min=-20.0, step=0.5), FloatSlider(val…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_gaussian(mu: float, sigma: float)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plotting Gaussian distrib\n",
    "ipywidgets.interact(plot_gaussian, mu=(-20,20,0.5), sigma=(0.1, 50, 0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545b130-591a-4e57-90b4-ef27e6c71fd7",
   "metadata": {},
   "source": [
    "It is straightforward to show that the Gaussian distribution is normalized, as expected from a proper probability distribution:\n",
    "$$\n",
    "\\int_{-\\infty}^{+\\infty} \\mathcal{N} (x|\\mu, \\sigma^2) dx = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\cdot \\sqrt{\\frac{\\pi}{\\beta/2}} \\equiv 1\n",
    "$$\n",
    "since $\\int_{-\\infty}^{+\\infty} \\exp{(-\\alpha x^2)} = \\sqrt{\\pi/a}$. Let verify that the `generate_gaussian` function defined above actually defines a proper distribution, computing numerically its integral by means of the `scipy.quad` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f38f6e4-d89a-4272-9b38-3fe005cd45bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "mu = 0\n",
    "sigma = 1\n",
    "\n",
    "# Integrate the Gaussian function from -infinity to +infinity\n",
    "integral, _ = quad(generate_gaussian, -np.inf, np.inf, args=(mu, sigma))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7143c69b-b73d-4c3b-b7a1-9a74b377f4d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The integral of the Gaussian distribution is: 0.9999999999999997\n"
     ]
    }
   ],
   "source": [
    "print(f\"The integral of the Gaussian distribution is: {integral}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146d5c08-3c8f-41ca-955f-d8491b5fcacf",
   "metadata": {},
   "source": [
    "Let now give a brief justification of the term *mean* and *variance*. We consider a real-value variable $x$ distributed according to a Gaussian distribution of mean $\\mu$ and standard deviation $\\sigma$. We can then compute the expectation value of $x$ under such Gaussian distribution as:\n",
    "$$\n",
    "\\mathbb{E}[x] = \\int_{-\\infty}^{+\\infty} \\mathcal{N}(x | \\mu, \\sigma^2) x dx\n",
    "$$\n",
    "We could perform the integration analytically but we skip such simple but boring calculations here. Instead, let compute the integral using again the `quad` method from `scipy`. We perform the numerical integration for different pairs of $(\\mu, \\sigma)$ and look at the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9743519b-b2cb-47fe-a55c-8d9fc73db230",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_expectation_integrand(x, mu, sigma):\n",
    "    return x*generate_gaussian(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adc49ba7-5a57-4705-9940-c163800509e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "mu_values = [0, 1, 1.5, 2, 2.5, 5, 10.5]\n",
    "sigma_values = [1, 1.5, 2, 2.5, 5, 10.5, 11]\n",
    "\n",
    "params = list(zip(mu_values, sigma_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d8b55e7-f836-4c37-aec1-26cf5c3a12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_dict = {}\n",
    "# Cycling over params and computing the expecation value\n",
    "for i, param in enumerate(params):\n",
    "    \n",
    "    mu, sigma = param\n",
    "\n",
    "    # Compute the Expectation value for given mean and standard devation\n",
    "    expectation_value, _ = quad(gaussian_expectation_integrand, -np.inf, np.inf, args=(mu, sigma))\n",
    "\n",
    "    expectation_dict[i] = {\n",
    "        \"mu\": mu, \n",
    "        \"sigma\": sigma,\n",
    "        \"expectation\": round(expectation_value, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "731b1114-67bd-438c-97aa-daa6eff297a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'expectation': 0.0, 'mu': 0, 'sigma': 1},\n",
      " 1: {'expectation': 1.0, 'mu': 1, 'sigma': 1.5},\n",
      " 2: {'expectation': 1.5, 'mu': 1.5, 'sigma': 2},\n",
      " 3: {'expectation': 2.0, 'mu': 2, 'sigma': 2.5},\n",
      " 4: {'expectation': 2.5, 'mu': 2.5, 'sigma': 5},\n",
      " 5: {'expectation': 5.0, 'mu': 5, 'sigma': 10.5},\n",
      " 6: {'expectation': 10.5, 'mu': 10.5, 'sigma': 11}}\n"
     ]
    }
   ],
   "source": [
    "pprint(expectation_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc40f8e-72ad-40d9-8ee5-16b75517205f",
   "metadata": {},
   "source": [
    "We can see that the expectation values, i.e. <span style=\"color: red;\">the average values of $x$ under the given distribution, are always equal to the *mean* $\\mu$ parameter of the Gaussian distribution</span>. That is why $\\mu$ is referred to as the *mean of the Gaussian distribution*. In probability theory, $\\mathbb{E}[x]$ is generally known as the **first order moment**. The **second-order moment** is defined as the expectation value of the square of $x$, i.e.:\n",
    "$$\n",
    "\\mathbb{E}[x^2] = \\int_{-\\infty}^{+\\infty} \\mathcal{N}(x | \\mu, \\sigma^2) x^2 dx\n",
    "$$\n",
    "Computing again the integral numerically for a set of possibile $\\mu$, $\\sigma$ parameters, we find:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62bb7296-93ae-4ec9-8b54-441febf7a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_second_order_moment_integrand(x, mu, sigma):\n",
    "    return (x**2)*generate_gaussian(x, mu, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36cab97c-dcb1-43f2-a09d-203e9eda6360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycling over params and computing the expecation value\n",
    "for i, param in enumerate(params):\n",
    "    \n",
    "    mu, sigma = param\n",
    "\n",
    "    # Compute the Expectation value for given mean and standard devation\n",
    "    second_order_moment, _ = quad(gaussian_second_order_moment_integrand, -np.inf, np.inf, args=(mu, sigma))\n",
    "\n",
    "    expectation_dict[i][\"second-order-moment\"] = round(second_order_moment, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e7cf18c-711b-4012-8660-661def96935d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'mu': 0, 'sigma': 1, 'expectation': 0.0, 'second-order-moment': 1.0},\n",
       " 1: {'mu': 1, 'sigma': 1.5, 'expectation': 1.0, 'second-order-moment': 3.25},\n",
       " 2: {'mu': 1.5, 'sigma': 2, 'expectation': 1.5, 'second-order-moment': 6.25},\n",
       " 3: {'mu': 2, 'sigma': 2.5, 'expectation': 2.0, 'second-order-moment': 10.25},\n",
       " 4: {'mu': 2.5, 'sigma': 5, 'expectation': 2.5, 'second-order-moment': 31.25},\n",
       " 5: {'mu': 5,\n",
       "  'sigma': 10.5,\n",
       "  'expectation': 5.0,\n",
       "  'second-order-moment': 135.25},\n",
       " 6: {'mu': 10.5,\n",
       "  'sigma': 11,\n",
       "  'expectation': 10.5,\n",
       "  'second-order-moment': 231.25}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expectation_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae1e9b0-6e6e-41aa-bdce-d8c606127f6a",
   "metadata": {},
   "source": [
    "It may not be evident but one can actually verify that the second order moment is equal to $\\mu^2+\\sigma^2$ (the diligent reader can verify the result by computing the integration analytically. The integral can be solved in few steps applying the integration by part rule!). Therefore, we have:\n",
    "$$\\begin{gather}\n",
    "\\mathbb{E}[x] = \\int_{-\\infty}^{+\\infty} \\mathcal{N}(x | \\mu, \\sigma^2) x^2 dx = \\mu \\\\\n",
    "\\mathbb{E}[x^2] = \\int_{-\\infty}^{+\\infty} \\mathcal{N}(x | \\mu, \\sigma^2) x^2 dx = \\mu^2+\\sigma^2\n",
    "\\end{gather}\n",
    "$$\n",
    "Then:\n",
    "$$\n",
    "\\text{var}[x] = \\mathbb{E}[x^2]-\\mathbb{E}[x]^2 = \\mu^2+\\sigma^2-\\mu^2 = \\sigma^2\n",
    "$$\n",
    "hence $\\sigma^2$ is called the *variance* of the Gaussian distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccb492a-385f-4535-931c-d9a43ba4d770",
   "metadata": {},
   "source": [
    "## The Likelihood Function for the Gaussian Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddbd07f-2b34-4805-86fc-2cf82d868371",
   "metadata": {},
   "source": [
    "Let now suppose to have a set of observations of a real-valued variable $x$, i.e. $\\boldsymbol{x} = (x_1, x_2, ..., x_N)^T$. We shall also suppose that each observation $x_i$ is drawn independently from a Gaussian distribution with mean $\\mu$ and variance $\\sigma^2$, which are unknown to us. We consider the problem of determining the values of $\\mu$ and $\\sigma^2$ from the data set. First of all, we must pay attention to the statement that data points are <span style=\"color: red;\"> drawn independently</span> from the distribution. Data points of this kind, which is sometimes abbreviated as **i.i.d** i.e. *independent and identically distributed*, allow us to write the probability of the data set in a rather simple and convenient form. Indeed, since the **joint** probability of two independent events is given by the product of the marginal probabilities for each event separately, we have:\n",
    "$$\n",
    "p(\\boldsymbol{x}|\\mu, \\sigma^2) = \\prod_{n=1}^N \\mathcal{N}(x_n|\\mu, \\sigma^2)\n",
    "$$\n",
    "In other words, the probability of the data set $\\boldsymbol{x}$ is the product of $N$ independent Gaussian distributions with mean $\\mu$ and variance $\\sigma^2$. When viewed as a function of such parameters, the above defined function is referred to as the **likelihood function for the Gaussian distribution**. \n",
    "\n",
    "In the so-called **maximum likelihood approach**, the problem translates into determining the values $\\mu$ and $\\sigma^2$ maximizing the likelihood function. It turns out that is much more convenient to maximize the $\\ln$ of the likelihood function (remind that this is equivalent, since the $\\log$ is a monotonically increasing function of its argument):\n",
    "$$\n",
    "\\ln p(\\boldsymbol{x}|\\mu, \\sigma^2) = -\\frac{1}{2\\sigma^2}\\sum_{n=1}^N (x_n -\\mu)^2 -\\frac{N}{2}\\ln \\sigma^2 - \\frac{N}{2} ln(2\\pi)\n",
    "$$\n",
    "Setting the derivative w.r.t. $\\mu$ to 0, we find:\n",
    "$$\n",
    "\\frac{\\partial \\ln p}{\\partial \\mu} = -\\frac{1}{2\\sigma^2}\\cdot 2 \\cdot \\sum_{n=1}^N x_n - \\frac{1}{2\\sigma^2}\\sum_{n=1}^N 2\\mu = 0\n",
    "$$\n",
    "i.e.\n",
    "$$\n",
    "\\mu_{ML} = \\frac{1}{N} \\sum_{n=1}^N x_n\n",
    "$$\n",
    "where the subscript $ML$ stands for \"maximum-likelihood\". The value $\\mu_{ML}$ is the *sample mean*, i.e. the mean of the observed values $x_n$. Similarly, we can maximize w.r.t. $\\sigma^2$, to find:\n",
    "$$\n",
    "\\sigma_{ML}^2 = \\frac{1}{N} \\sum_{n=1}^N (x_n-\\mu_{ML})^2\n",
    "$$\n",
    "which is referred to as the *sample variance*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5fae52-b2d4-40bc-bc0f-618c82a626ce",
   "metadata": {},
   "source": [
    "To some extent, we may say that the sample mean and the sample variance are best estimates for the true Gaussian distribution parameters in the maximum-likelihood approach. However, this approach has some limitations. Indeed, if we compute the expecation value of $\\mu_{ML}$ and $\\sigma^2_{ML}$ under the Gaussian distribution, we obtain:\n",
    "$$\n",
    "\\begin{gather}\n",
    "\\mathbb{E}[\\mu_{ML}] = \\mu \\\\\n",
    "\\mathbb{E}[\\sigma^2_{ML}] = \\left(\\frac{N-1}{N}\\right)\\sigma^2\n",
    "\\end{gather}\n",
    "$$\n",
    "Therefore, <span style=\"color: red;\">the maximization of the likelihood function will give the right estimate for the mean but it will sistematically underestimate the true variance by a factor of $(N-1)/N$</span>. Such problem is known as **bias** and is related to a well known problem in Machine Learning called **over-fitting**. We note that in the limit:\n",
    "$$\n",
    "\\lim_{N\\to +\\infty} \\sigma^2_{ML} = \\sigma^2\n",
    "$$\n",
    "therefore, the bias problem becomes less significant. In the following, we play a little bit with Python to see these concepts in action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81147a-1e52-481a-81ab-bb98fa51fcc5",
   "metadata": {},
   "source": [
    "Let define a function to generate N data points drawn indpendent from a Gaussian distribution. We exploit the `numpy.random.normal`function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1f18fe5-83a0-4712-a9b6-2ccbac6e1d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_univariate_gaussian_data(N: int, mu: float, sigma: float) -> np.array:\n",
    "    # Generate N data points with one variable (x) distributed according to a Gaussian distribution\n",
    "    return np.random.normal(mu, sigma, N)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83634ec6-8df2-490e-9264-9b9ad98f52ff",
   "metadata": {},
   "source": [
    "To generate a \"true\" Gaussian distribution we use the function defined above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b9687d74-3d0c-48a8-8a7d-37b912948e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gaussian_data_vs_true_gaussian(N: int, mu: float):\n",
    "\n",
    "    # We set the standard deviation to 1\n",
    "    sigma=1.00\n",
    "    \n",
    "    # Drawn data points from a Gaussian distribution of known parameters\n",
    "    data_points = generate_univariate_gaussian_data(N, mu, sigma)\n",
    "\n",
    "    # Compute the sample mean of data points:\n",
    "    sample_mean = round(np.mean(data_points), 2)\n",
    "    sample_variance = round(np.mean((data_points-sample_mean)**2), 2)\n",
    "\n",
    "    print(f\"Δ(μ - μ(ML)) = {round(abs(mu - sample_mean), 2)}\")\n",
    "    print(f\"Δ(σ² - σ²(ML)) = {round(abs(sigma**2 - sample_variance), 2)}\")\n",
    "    \n",
    "    # Plot the data points distribution as well as the true underlying Gaussian Distribution\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "\n",
    "    # histogram of data pints\n",
    "    ax.hist(data_points, density=True, color=\"royalblue\", edgecolor=\"blue\", alpha=0.35, label=\"Data Points\")\n",
    "    ax.axvline(x=sample_mean, color=\"red\", linestyle=\"--\", label=\"Sample Mean\")\n",
    "\n",
    "    # underlying Gaussian distribution\n",
    "    x_values = np.linspace(min(data_points), max(data_points), 1000)\n",
    "    ax.plot(x_values, generate_gaussian(x_values, mu, sigma), color=\"blue\", label=\"Gaussian Distribution\")\n",
    "    ax.axvline(x=mu, color=\"darkorange\", linestyle=\"--\", label=\"True Mean\")\n",
    "\n",
    "    ax.set_title('Data Points drawn from Gaussian Distribution')\n",
    "    ax.set_xlabel(r'$x$')\n",
    "    ax.set_ylabel(r'$p(x)$')\n",
    "\n",
    "    ax.legend()\n",
    "        \n",
    "    ax.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5f4bc9d-2e47-4edf-927a-e87b3c91af63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50c30be22a0946278f943e76a00754c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1010, description='N', max=2000, min=20), FloatSlider(value=0.0, descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.plot_gaussian_data_vs_true_gaussian(N: int, mu: float)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate Bias\n",
    "ipywidgets.interact(plot_gaussian_data_vs_true_gaussian, N=(20, 2000, 1), mu=(-5,5,0.50))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8309820-fa92-448c-addd-aa3c1070e55c",
   "metadata": {},
   "source": [
    "Notice that above we are performing just a \"single\" experiment here, therefore the difference between the sample mean and the true mean may vary and be different from 0, especially from small $N$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7590e36-d8b5-4962-aaed-0a981e164cc1",
   "metadata": {},
   "source": [
    "## The Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e495fca0-3d05-44e2-a15f-0efcf66448b9",
   "metadata": {},
   "source": [
    "The reason why the Gaussian distribution is so important and plays a central role in the whole probabily theory comes from the fact that it arises in many different contexts and can be motivated from a variety of perspectives. For instance, it can be showed that the Gaussian distribution is the probability distribution which maximizes the *entropy* of a single real variable (we will cover this topic in another Jupyter Notebook!). Here we would like to motivate the Gaussian distribution following a rather standard approach in probability theory, namely discussing the so-called **Central Limit Theorem**. Let state the theorem in its classical form:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d6565-e168-49e3-a82d-8fb3021cd7f9",
   "metadata": {},
   "source": [
    "**Lindeberg–Lévy Central Limit Theorem**: Suppose that $x_1, x_2, ..., x_N$ is a sequence of *independent and identically distributed* variables with expected value $\\mathbb{E}[x_i] = \\mu$ and variance $\\text{var}[x_i] = \\sigma^2$. Then, in the limit $N\\to +\\infty$, the distribution of the mean $(x_1+x_2+...x_N)/N$ converge to a *Gaussian distribution* of mean $\\mu$ and variance $\\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\bar{x}_N \\to \\mathcal{N}(\\mu, \\sigma^2)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b256f9-b170-4e4a-b2fe-b11d7ec09fee",
   "metadata": {},
   "source": [
    "A mathematical proof of the theorem is out of the scope of the current notebook. However, in the following we write some code to see the central limit theorem in action. In particular, we define a number of samples of $N$ random number uniformly distributed in the interval $[0,1]$ and we plot the distribution of the sample means. We will see that increasing the number of random numbers $N$, the distribution of sample means will tend to a Gaussian distribution. \n",
    "\n",
    "We can generate $S$ samples, each of size $N$ with $N$ uniformly distributed numbers using the `numpy.random.rand`function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ba3b9de-526e-4861-9fc4-714de93d74db",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples = np.random.rand(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db63ee4-7481-4ed9-80fb-496b0060e209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.74161483, 0.47014132],\n",
       "        [0.28312363, 0.57181479],\n",
       "        [0.65645723, 0.87219163],\n",
       "        [0.60421014, 0.86737384],\n",
       "        [0.41623246, 0.87405377],\n",
       "        [0.18990754, 0.88994352],\n",
       "        [0.33173109, 0.42982442],\n",
       "        [0.05518175, 0.3359833 ],\n",
       "        [0.4098769 , 0.76023737],\n",
       "        [0.61006243, 0.91122542]]),\n",
       " (10, 2))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_samples, test_samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679277be-4e36-4ec1-9ec7-023bff2df4a4",
   "metadata": {},
   "source": [
    "We see that `test_samples` is a 2D Numpy Array of shape (10, 2), i.e. 10 rows each of 2 columns. This represents 10 samples of 2 numbers. The sample means can be stored in a numpy array of shape (10,) and can be obtained computing means across the `axis=1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b33e8e15-4a5b-4f40-b339-28cb6ee390d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.60587807, 0.42746921, 0.76432443, 0.73579199, 0.64514311,\n",
       "        0.53992553, 0.38077775, 0.19558252, 0.58505714, 0.76064392]),\n",
       " (10,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means = np.mean(test_samples, axis=1)\n",
    "means, means.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e3f464-7186-413f-a9e6-d2fc79139fa9",
   "metadata": {},
   "source": [
    "Let's wrap everything into a function that can be plotted interactively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce793c4f-0671-467f-a2e9-582a37e26ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_limit_theorem_demo(N: int):\n",
    "    # Fixing the number of samples to a large number\n",
    "    n_samples = 10000\n",
    "\n",
    "    # Computing sample means\n",
    "    sample_means = np.mean(np.random.rand(n_samples, N), axis=1)\n",
    "\n",
    "    # Plot histograms of sample means\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    fig.suptitle('Central Limit Theorem Demonstration')\n",
    "\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.hist(sample_means, bins=30, density=True, alpha=0.35, color='royalblue', edgecolor='blue')\n",
    "    ax.set_title(f'Distribution of Sample Means (N = {N})')\n",
    "    ax.set_xlabel('Sample Mean')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06fa983e-aa01-42a4-af9d-3893a7b5126a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f709d699c24463cb9d270a6194fe91c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=25, description='N', max=50, min=1), Output()), _dom_classes=('widget-in…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.central_limit_theorem_demo(N: int)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate Central Limit Theorem\n",
    "ipywidgets.interact(central_limit_theorem_demo, N=(1,50,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec48c13-e1d2-4288-bcdf-aa69dac3a360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f277eae-a13e-4d4a-9b8b-6188f846166a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
