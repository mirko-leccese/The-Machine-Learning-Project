{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cca0baf8-af09-4eac-9ac9-f91f42a5d4f7",
   "metadata": {},
   "source": [
    "# An informal Introduction to Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04031728-b264-4077-8e3f-c283d2265564",
   "metadata": {},
   "source": [
    "In this notebook, we provide a relatively informal introduction the concept of **probability distribution*. The significance of this subject lies in its foundational role across various disciplines (data science as well as scientic subjects such as physics or chemistry), where understanding the probability distribution shapes of data or phenomena under examination serves as a starting point for most applications. There are lots of mathematical details that could be covered when talking about probability distributions, since the subject is vast and can be discussed under different persepctives. The aim of this notebook is to provide just an introductory overview, but without compromising too heavily on the mathematical aspects. Specifically, we will delve into some of the most prevalent probability distributions and their properties, such as the *Gaussian* or *Beta* distribution. Additionally, we will include Python simulations to facilitate a better grasp of the concepts discussed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a3eaca-238c-4237-b951-0bb3241c38ec",
   "metadata": {},
   "source": [
    "## Probability Densities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf681e03-8655-4f78-ae5b-a83e7af0e788",
   "metadata": {},
   "source": [
    "In a *frequentistic* approach, we define the probability of an event as a measure of the frequency for the event to occur, in the limit that the total number of trials goes to infinity. For instance, the probabilty of rolling a dice and get \"head\" is 1/2, since in the limit of an infinite number of trials, we should expect to get \"head\" half of the time and \"tail\" the other half. Such probabilities are defined over discrete sets of events, in the provided example $\\{H, T\\}$, i.e. \"head\" or \"tail\". However, most of the time we deal with continuous variables and we need to extend probabilities to such a continiuous case. \n",
    "\n",
    "If $x$ is a real-value continious variable, the *probability density* over $x$ is the defined as the quantity $p(x)$ such that $p(x)\\delta x$ is the probability for $x$ to fall in the interval $(x, x+\\delta x)$, in the limit $\\delta x \\to 0$. Then, we may define:\n",
    "\n",
    "$$P(x \\in (a, b)) = \\int_a^b p(x) dx $$\n",
    "\n",
    "which is the probability for $x$ to fall in the interval $(a, b)$. The probability density $p(x)$ must satisfy the following two conditions:\n",
    "$$ \\begin{gather}\n",
    "p(x) \\geq 0 \\\\\n",
    "\\int_{-\\infty}^{+\\infty} p(x) dx = 1\n",
    "\\end{gather}\n",
    "$$\n",
    "The sum and product rules, as well as Beyes' Theorem, apply equally to the case of probability densities. If $x$ and$y$ are two real variables, then the product and sum rules take the form:\n",
    "$$\\begin{gather}\n",
    "p(x) = \\int p(x,y) dy \\\\\n",
    "p(x,y) = p(y|x) p(x)\n",
    "\\end{gather}\n",
    "$$\n",
    "where $p(y|x)$ is the *conditional probability* of y given x. \n",
    "In analogy with the discrete case, we may define the average value of some function $f(x)$, assuming that  $x$ follows the probability distributions $p(x)$, as:\n",
    "$$\n",
    "\\mathbb{E}[f] = \\int p(x) f(x) dx\n",
    "$$\n",
    "The operator $\\mathbb{E}(\\cdot)$ is generally known as the **expectation** (of $f(x)$). The **variance** of $f(x)$ is instead a quantity measuring the variability in $f(x)$ around its mean value $\\mathbb{E}[f(x)]$ and is defined as:\n",
    "$$\n",
    "\\text{var}[f] = \\mathbb{E}[(f(x)-\\mathbb{E}[f(x)])^2]\n",
    "$$\n",
    "in other words, it is the expectation value of the squared difference between $f(x)$ and its mean value. We take the *squared* difference since the expected value of the mere difference $f(x) - \\mathbb{E}[f(x)]$ is identically vanishing. Working out the expected value and reminding that $\\mathbb{E}$ is a *linear* operator, we can re-write the variance in a pretty simple form: \n",
    "$$\\begin{align}\n",
    "\\text{var}[f(x)] &= \\mathbb{E}[f(x)^2 -2f(x)E[f(x)] + (\\mathbb{E}[f(x)])^2] \\\\\n",
    "&=\\mathbb{E}[f(x)^2] - 2(\\mathbb{E}[f(x)])^2+(\\mathbb{E}[f(x)])^2 \\\\\n",
    "&= \\mathbb{E}[f(x)^2]-\\mathbb{E}[f(x)]^2\n",
    "\\end{align}\n",
    "$$\n",
    "For two random variables $x$ and $y$ we define also the **covariance** as:\n",
    "$$\\begin{align}\n",
    "\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n",
    "&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n",
    "\\end{align}\n",
    "$$\n",
    "which expresses the extent to which $x$ and $y$ vary together (i.e. \"co-vary\"). Indeed, if $x$ and $y$ are independent variables, then:\n",
    "$$\\begin{align}\n",
    "\\mathbb{E}_{x,y}[xy] &= \\int \\int xy p(x)p(y) dxdy \\\\\n",
    "&= \\left( \\int x p(x) dx)\\right) \\left( \\int y p(y) dy\\right) \\\\\n",
    "&= \\mathbb{E}_x[x]\\mathbb{E}_y[y]\n",
    "\\end{align}\n",
    "$$\n",
    "hence their covariance vanishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "229e0416-b855-45d2-a0c5-ed7cdaf5eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4607e7e5-d1c0-4799-b903-813f77568eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8489c9d-0204-403d-be5e-95935058c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce793c4f-0671-467f-a2e9-582a37e26ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def central_limit_theorem_demo(N):\n",
    "    sample_size = 10000\n",
    "    \n",
    "    sample_means = np.mean(np.random.rand(sample_size, N), axis=1)\n",
    "\n",
    "    # Plot histograms of sample means\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    fig.suptitle('Central Limit Theorem Demonstration')\n",
    "\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.hist(sample_means, bins=30, density=True, alpha=0.6, color='royalblue', edgecolor='black')\n",
    "    ax.set_title(f'Distribution of Sample Means (N = {N})')\n",
    "    ax.set_xlabel('Sample Mean')\n",
    "    ax.set_ylabel('Density')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06fa983e-aa01-42a4-af9d-3893a7b5126a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f7eeab48c849c48ca9b8dc86c0b7fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=49, description='N', min=1, step=2), Output()), _dom_classes=('widget-inâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.central_limit_theorem_demo(N)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demonstrate Central Limit Theorem\n",
    "ipywidgets.interact(central_limit_theorem_demo, N=(1,100,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec48c13-e1d2-4288-bcdf-aa69dac3a360",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
